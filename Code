package ai.certifai.tryanderror;

import org.datavec.api.records.reader.impl.collection.CollectionRecordReader;
import org.datavec.api.records.reader.impl.csv.CSVRecordReader;
import org.datavec.api.split.FileSplit;
import org.datavec.api.transform.TransformProcess;
import org.datavec.api.transform.schema.Schema;
import org.datavec.api.writable.Writable;
import org.datavec.local.transforms.LocalTransformExecutor;
import org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.layers.DenseLayer;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.nd4j.common.io.ClassPathResource;
import org.nd4j.evaluation.classification.Evaluation;
import org.nd4j.linalg.activations.Activation;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.SplitTestAndTrain;
import org.nd4j.linalg.dataset.ViewIterator;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.dataset.api.preprocessor.DataNormalization;
import org.nd4j.linalg.dataset.api.preprocessor.NormalizerMinMaxScaler;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.learning.config.Adam;
import org.nd4j.linalg.lossfunctions.impl.LossBinaryXENT;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

public class heart {

    static int seed = 36;
    static Double splitRatio = 0.8;
    static int batchSize = 12;
    static int nEpoch = 1;
    static double lr = 1e-3;

    public static void main(String[] args) throws IOException, InterruptedException {

        File trainFile = new ClassPathResource("midTermQuiz/Heart/heart_train.csv").getFile();

        CSVRecordReader rr = new CSVRecordReader(1,',');
        rr.initialize(new FileSplit(trainFile));

        Schema schema = new Schema.Builder()
                .addColumnsInteger("age","sex","trtbps","chol","thalachh")
                .addColumnCategorical("cp","0","1","2","3")
                .addColumnCategorical("fbs","0","1")
                .addColumnCategorical("restecg","0","1")
                .addColumnCategorical("exng","0","1")
                .addColumnDouble("oldpeak")
                .addColumnCategorical("slp","0","1")
                .addColumnCategorical("caa","0","1","2","3","4")
                .addColumnCategorical("thall","0","1","2","3")
                .addColumnCategorical("output","0","1")
                .build();
        System.out.println("Initial Schema: "+ schema);

       TransformProcess tp = new TransformProcess.Builder(schema)
               .categoricalToInteger("output")
               .categoricalToOneHot("cp","fbs","exng","slp","caa","thall")
               .build();

        System.out.println("Final Schema: "+ tp.getFinalSchema());

        List<List<Writable>> allData = new ArrayList<>();
        while(rr.hasNext()){
            allData.add(rr.next());
        }

        List<List<Writable>> transformedData = LocalTransformExecutor.execute(allData,tp);
        CollectionRecordReader crr = new CollectionRecordReader(transformedData);

        DataSetIterator iter = new RecordReaderDataSetIterator(crr, transformedData.size(),26,2);

        DataSet fullDataSet = iter.next();
        fullDataSet.shuffle(seed);

        SplitTestAndTrain trainAndVal = fullDataSet.splitTestAndTrain(splitRatio);
        DataSet trainSet = trainAndVal.getTrain();
        DataSet valSet = trainAndVal.getTest();

        System.out.println("Training vector : ");
        System.out.println(Arrays.toString(trainSet.getFeatures().shape()));
        System.out.println("Validation vector : ");
        System.out.println(Arrays.toString(valSet.getFeatures().shape()));
        DataNormalization scaler = new NormalizerMinMaxScaler();
        scaler.fit(trainSet);
        scaler.transform(trainSet);
        scaler.transform(valSet);

        DataSetIterator trainiter = new ViewIterator(trainSet, batchSize);
        DataSetIterator valIter = new ViewIterator(valSet, batchSize);

        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
                .seed(seed)
                .updater(new Adam(lr))
                .weightInit(WeightInit.XAVIER)
                .list()
                .layer(new DenseLayer.Builder()
                        .nIn(trainSet.numInputs())
                        .nOut(48)
                        .activation(Activation.RELU)
                        .build())
                .layer(new DenseLayer.Builder()
                        .nIn(48)
                        .nOut(36)
                        .activation(Activation.RELU)
                        .build())
                .layer(new DenseLayer.Builder()
                        .nIn(36)
                        .nOut(24)
                        .activation(Activation.RELU)
                        .build())
                .layer(new OutputLayer.Builder()
                        .nIn(24)
                        .nOut(trainSet.numOutcomes())
                        .activation(Activation.SIGMOID)
                        .lossFunction(new LossBinaryXENT(Nd4j.create(new double[]{0.8,0.5})))
                        .build())
                .build();

        MultiLayerNetwork model = new MultiLayerNetwork(conf);
        model.init();

        model.setListeners(new ScoreIterationListener(10));

        model.fit(trainiter, nEpoch);

        Evaluation evalTrain = model.evaluate(trainiter);
        Evaluation evalValidation = model.evaluate(valIter);

        System.out.println("Training Evaluation: " + evalTrain.stats());
        System.out.println("Validation Evaluation: " + evalValidation.stats());

    }
}
